{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFN6iHjK60s60kdl29tK5/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethandlouiee/MGMT467_Team11/blob/main/team/Final_Project/pipeline/MGMT467_FinalProject_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 - Batch Ingestion:**"
      ],
      "metadata": {
        "id": "wZa1wAPCaj4q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1rpQK39EZVu",
        "outputId": "6ce42bc4-f5ae-4d82-bc87-f6ca042126db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed and Google Cloud successfully authenticated!\n"
          ]
        }
      ],
      "source": [
        "# @title Step 1.1: Setup & Authentication\n",
        "# Install required libraries for Google Cloud and Kaggle\n",
        "!pip install -q kaggle google-cloud-bigquery google-cloud-storage pandas db-dtypes\n",
        "\n",
        "# Authenticate User for Google Cloud access\n",
        "# This will trigger a popup to allow access to your GCP resources\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "print(\"Libraries installed and Google Cloud successfully authenticated!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1.2: Configuration & Kaggle JSON Upload\n",
        "import os\n",
        "import json\n",
        "from google.colab import files\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "\n",
        "# --- Google Cloud Config ---\n",
        "# User Input for Project ID\n",
        "project_id = \"mgmt467-fp-11\" # @param {type:\"string\"}\n",
        "region = \"us-central1\" # @param {type:\"string\"}\n",
        "\n",
        "# Define Resource Names\n",
        "bucket_name = f\"air_quality_raw_{project_id}\" # Unique bucket name\n",
        "dataset_name = \"air_quality_dataset\"\n",
        "table_name = \"sensor_data\"\n",
        "\n",
        "# Set the environment variable for the project\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project_id\n",
        "\n",
        "# Initialize Clients\n",
        "bq_client = bigquery.Client(project=project_id)\n",
        "storage_client = storage.Client(project=project_id)\n",
        "\n",
        "print(f\"Configuration set for Project: {project_id}\")\n",
        "print(f\"Target Bucket: {bucket_name}\")\n",
        "print(\"-------------------------------------------------\")\n",
        "\n",
        "# --- Kaggle Authentication ---\n",
        "print(\"Please upload your kaggle.json file now...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Check if kaggle.json was uploaded\n",
        "if 'kaggle.json' in uploaded:\n",
        "    # Create the .kaggle directory if it doesn't exist\n",
        "    !mkdir -p ~/.kaggle\n",
        "\n",
        "    # Move the uploaded file to the .kaggle directory\n",
        "    !mv kaggle.json ~/.kaggle/\n",
        "\n",
        "    # Change permissions to ensure the file is secure (required by Kaggle API)\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    print(\"\\nSUCCESS: kaggle.json uploaded and permissions set.\")\n",
        "else:\n",
        "    print(\"\\nERROR: kaggle.json not found. Please run the cell again and upload the correct file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "lwiqQfMkFbRt",
        "outputId": "445273f6-af00-4848-de98-5a63c6f037bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration set for Project: mgmt467-fp-11\n",
            "Target Bucket: air_quality_raw_mgmt467-fp-11\n",
            "-------------------------------------------------\n",
            "Please upload your kaggle.json file now...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1691b550-d47a-4ab0-9f72-b258135470a8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1691b550-d47a-4ab0-9f72-b258135470a8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "\n",
            "SUCCESS: kaggle.json uploaded and permissions set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1.3: Ingest from Kaggle to GCS (Raw Data Lake)\n",
        "from google.cloud import storage\n",
        "import os\n",
        "\n",
        "# 1. Download Dataset from Kaggle\n",
        "print(\"Downloading data from Kaggle...\")\n",
        "!kaggle datasets download -d fedesoriano/air-quality-data-set\n",
        "\n",
        "# 2. Unzip the file\n",
        "print(\"Unzipping data...\")\n",
        "!unzip -o air-quality-data-set.zip\n",
        "\n",
        "# Find the .csv file name (it varies sometimes)\n",
        "files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
        "if not files:\n",
        "    raise ValueError(\"No CSV file found in the downloaded dataset!\")\n",
        "source_file_name = files[0]\n",
        "print(f\"Found raw file: {source_file_name}\")\n",
        "\n",
        "# 3. Create GCS Bucket (if it doesn't exist)\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "if not bucket.exists():\n",
        "    print(f\"Creating bucket {bucket_name}...\")\n",
        "    bucket = storage_client.create_bucket(bucket_name, location=region)\n",
        "else:\n",
        "    print(f\"Bucket {bucket_name} already exists.\")\n",
        "\n",
        "# 4. Upload File to GCS\n",
        "blob_name = source_file_name # Keep the same name in GCS\n",
        "blob = bucket.blob(blob_name)\n",
        "\n",
        "print(f\"Uploading {source_file_name} to gs://{bucket_name}/{blob_name}...\")\n",
        "blob.upload_from_filename(source_file_name)\n",
        "\n",
        "print(\"Success! Raw data is now stored in Google Cloud Storage.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4ZbzXNYGfZF",
        "outputId": "f4c6720f-2ab5-433c-b404-59abb1c5a0c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from Kaggle...\n",
            "Dataset URL: https://www.kaggle.com/datasets/fedesoriano/air-quality-data-set\n",
            "License(s): copyright-authors\n",
            "Downloading air-quality-data-set.zip to /content\n",
            "  0% 0.00/248k [00:00<?, ?B/s]\n",
            "100% 248k/248k [00:00<00:00, 568MB/s]\n",
            "Unzipping data...\n",
            "Archive:  air-quality-data-set.zip\n",
            "  inflating: AirQuality.csv          \n",
            "Found raw file: AirQuality.csv\n",
            "Bucket air_quality_raw_mgmt467-fp-11 already exists.\n",
            "Uploading AirQuality.csv to gs://air_quality_raw_mgmt467-fp-11/AirQuality.csv...\n",
            "Success! Raw data is now stored in Google Cloud Storage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1.4: Curated Load to BigQuery (Schema & Partitioning)\n",
        "import pandas as pd\n",
        "import re\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# 1. Read Raw Data from GCS\n",
        "source_uri = f\"gs://{bucket_name}/{source_file_name}\"\n",
        "print(f\"Reading raw data from {source_uri}...\")\n",
        "df = pd.read_csv(source_uri, sep=';', decimal=',')\n",
        "\n",
        "# 2. Data Curation\n",
        "# Drop empty columns (artifacts)\n",
        "df = df.dropna(axis=1, how='all')\n",
        "\n",
        "# Fix Date Format for Partitioning (DD/MM/YYYY -> YYYY-MM-DD)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y').dt.date\n",
        "\n",
        "# --- FIX: Robust Column Renaming ---\n",
        "# Function to clean column names for BigQuery (only Alphanumeric and _)\n",
        "def clean_col_name(name):\n",
        "    # Replace non-alphanumeric characters (like . or () ) with _\n",
        "    clean = re.sub(r'[^a-zA-Z0-9]', '_', name)\n",
        "    return clean\n",
        "\n",
        "# Apply cleaning\n",
        "df.columns = [clean_col_name(c) for c in df.columns]\n",
        "\n",
        "print(\"Cleaned Column Names:\", df.columns.tolist())\n",
        "\n",
        "# 3. Define BigQuery Schema & Partitioning\n",
        "# We map the specific clean names to types\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    schema=[\n",
        "        bigquery.SchemaField(\"Date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"Time\", \"STRING\"),\n",
        "        # We explicitly map the cleaned names:\n",
        "        bigquery.SchemaField(\"CO_GT_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"PT08_S1_CO_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"NMHC_GT_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"C6H6_GT_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"PT08_S2_NMHC_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"NOx_GT_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"PT08_S3_NOx_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"NO2_GT_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"PT08_S4_NO2_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"PT08_S5_O3_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"T\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"RH\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"AH\", \"FLOAT\"),\n",
        "    ],\n",
        "    # --- PARTITIONING ---\n",
        "    time_partitioning=bigquery.TimePartitioning(\n",
        "        type_=bigquery.TimePartitioningType.DAY,\n",
        "        field=\"Date\"\n",
        "    ),\n",
        "    write_disposition=\"WRITE_TRUNCATE\",\n",
        ")\n",
        "\n",
        "# 4. Load to BigQuery\n",
        "dataset_ref = bq_client.dataset(dataset_name)\n",
        "table_ref = dataset_ref.table(table_name)\n",
        "\n",
        "# Ensure dataset exists\n",
        "try:\n",
        "    bq_client.get_dataset(dataset_ref)\n",
        "except:\n",
        "    bq_client.create_dataset(dataset_ref)\n",
        "\n",
        "print(f\"Loading data into {project_id}.{dataset_name}.{table_name}...\")\n",
        "job = bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
        "job.result()\n",
        "\n",
        "print(\"Success! Data loaded with Cleaned Schema and Partitioning.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1kdarYnG44i",
        "outputId": "d7def51c-ad36-4e92-e9b6-8fb8e1cf8aab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading raw data from gs://air_quality_raw_mgmt467-fp-11/AirQuality.csv...\n",
            "Cleaned Column Names: ['Date', 'Time', 'CO_GT_', 'PT08_S1_CO_', 'NMHC_GT_', 'C6H6_GT_', 'PT08_S2_NMHC_', 'NOx_GT_', 'PT08_S3_NOx_', 'NO2_GT_', 'PT08_S4_NO2_', 'PT08_S5_O3_', 'T', 'RH', 'AH']\n",
            "Loading data into mgmt467-fp-11.air_quality_dataset.sensor_data...\n",
            "Success! Data loaded with Cleaned Schema and Partitioning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1.5: Data Quality Check\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# --- 1. Data Quality Check (SQL) ---\n",
        "# We check for the specific error code '-200' which indicates sensor failure/missing data\n",
        "dq_query = f\"\"\"\n",
        "    SELECT\n",
        "        COUNT(*) as total_rows,\n",
        "        COUNTIF(CO_GT_ = -200) as missing_co_readings,\n",
        "        ROUND((COUNTIF(CO_GT_ = -200) / COUNT(*)) * 100, 2) as percent_missing\n",
        "    FROM `{project_id}.{dataset_name}.{table_name}`\n",
        "\"\"\"\n",
        "\n",
        "print(\"Running Data Quality Check on BigQuery...\")\n",
        "query_job = bq_client.query(dq_query)\n",
        "results = query_job.result()\n",
        "\n",
        "print(\"\\n--- Data Quality Results ---\")\n",
        "for row in results:\n",
        "    print(f\"Total Rows Loaded: {row.total_rows}\")\n",
        "    print(f\"Rows with Missing CO Data (-200): {row.missing_co_readings}\")\n",
        "    print(f\"Data Quality Impact: {row.percent_missing}% of Carbon Monoxide readings are missing.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ho1Ibw1LVhIa",
        "outputId": "dcb93e2f-8f6c-4717-f376-5cd34d9062f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Data Quality Check on BigQuery...\n",
            "\n",
            "--- Data Quality Results ---\n",
            "Total Rows Loaded: 9471\n",
            "Rows with Missing CO Data (-200): 1683\n",
            "Data Quality Impact: 17.77% of Carbon Monoxide readings are missing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation Logic Explanation\n",
        "\n",
        "Transformation: Date Format Conversion (DD/MM/YYYY -> YYYY-MM-DD)\n",
        "\n",
        "Logic:\n",
        "The raw CSV provided dates as strings in European format (e.g., '10/03/2004').\n",
        "However, BigQuery 'Time Partitioning' strictly requires a DATE or TIMESTAMP data type.\n",
        "We parsed the string into a Python Date object during the Pandas load step.\n",
        "\n",
        "Benefit:\n",
        "By transforming this column into a DATE type, we enabled BigQuery to physically\n",
        "partition the storage by Day. This reduces query costs and improves performance\n",
        "when filtering by specific dates, as the engine only scans relevant partitions\n",
        "rather than the entire table."
      ],
      "metadata": {
        "id": "grvwIVRzV4Q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 - Streaming Ingestion:**"
      ],
      "metadata": {
        "id": "bw1UIoXGapx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.1: Configure gcloud, Enable APIs & Create Pub/Sub\n",
        "# 1. Set the project for gcloud commands\n",
        "!gcloud config set project $project_id\n",
        "\n",
        "# 2. Enable APIs\n",
        "print(\"Enabling necessary APIs (this may take a minute)...\")\n",
        "!gcloud services enable \\\n",
        "    cloudfunctions.googleapis.com \\\n",
        "    run.googleapis.com \\\n",
        "    pubsub.googleapis.com \\\n",
        "    cloudbuild.googleapis.com \\\n",
        "    artifactregistry.googleapis.com --project=$project_id\n",
        "\n",
        "print(\"APIs enabled.\")\n",
        "\n",
        "# 3. Create Pub/Sub Topic\n",
        "topic_id = \"openaq-topic\"\n",
        "\n",
        "# Create the topic (using quiet flag to suppress 'already exists' error cleanly)\n",
        "!gcloud pubsub topics create $topic_id --project=$project_id || echo \"Topic likely already exists\"\n",
        "\n",
        "print(f\"\\nTarget Topic: projects/{project_id}/topics/{topic_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV3-MuYFaylO",
        "outputId": "de3074dd-5b07-4998-b5d9-58bf123611a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "\n",
            "\n",
            "To take a quick anonymous survey, run:\n",
            "  $ gcloud survey\n",
            "\n",
            "Enabling necessary APIs (this may take a minute)...\n",
            "\n",
            "\n",
            "Command killed by keyboard interrupt\n",
            "\n",
            "^C\n",
            "APIs enabled.\n",
            "\u001b[1;31mERROR:\u001b[0m Failed to create topic [projects/mgmt467-fp-11/topics/openaq-topic]: Resource already exists in the project (resource=openaq-topic).\n",
            "\u001b[1;31mERROR:\u001b[0m (gcloud.pubsub.topics.create) Failed to create the following: [openaq-topic].\n",
            "Topic likely already exists\n",
            "\n",
            "Target Topic: projects/mgmt467-fp-11/topics/openaq-topic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.2: Create Streaming Table & BigQuery Subscription\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# --- 1. Create BigQuery Streaming Table ---\n",
        "streaming_table_name = \"streaming_air_quality\"\n",
        "table_ref = bq_client.dataset(dataset_name).table(streaming_table_name)\n",
        "\n",
        "# Define Schema (Data fields only)\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
        "    bigquery.SchemaField(\"city\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"parameter\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"value\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"unit\", \"STRING\"),\n",
        "]\n",
        "\n",
        "table = bigquery.Table(table_ref, schema=schema)\n",
        "\n",
        "# Create table if it doesn't exist\n",
        "try:\n",
        "    bq_client.get_table(table_ref)\n",
        "    print(f\"Table {streaming_table_name} already exists.\")\n",
        "except:\n",
        "    print(f\"Creating table {streaming_table_name}...\")\n",
        "    bq_client.create_table(table)\n",
        "\n",
        "# --- 2. Grant Permissions ---\n",
        "print(\"Retrieving Project Number...\")\n",
        "project_number_list = !gcloud projects list --filter=\"project_id:$project_id\" --format=\"value(projectNumber)\"\n",
        "project_number = project_number_list[0]\n",
        "service_account = f\"service-{project_number}@gcp-sa-pubsub.iam.gserviceaccount.com\"\n",
        "\n",
        "# Grant permission\n",
        "!gcloud projects add-iam-policy-binding $project_id \\\n",
        "    --member=\"serviceAccount:$service_account\" \\\n",
        "    --role=\"roles/bigquery.dataEditor\" > /dev/null\n",
        "\n",
        "# --- 3. Create Subscription (Data Only) ---\n",
        "subscription_id = \"openaq-to-bq\"\n",
        "full_table_id = f\"{project_id}:{dataset_name}.{streaming_table_name}\"\n",
        "\n",
        "print(f\"Creating Subscription {subscription_id}...\")\n",
        "\n",
        "# We removed '--write-metadata' so it matches our table schema perfectly\n",
        "!gcloud pubsub subscriptions create $subscription_id \\\n",
        "    --topic=$topic_id \\\n",
        "    --bigquery-table=$full_table_id \\\n",
        "    --use-table-schema \\\n",
        "    --project=$project_id || echo \"Subscription likely already exists.\"\n",
        "\n",
        "print(f\"\\nPipeline Ready: Topic -> Subscription -> Table ({streaming_table_name})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU7Ha7TNcRt5",
        "outputId": "a0b679eb-9094-4bb6-e2f8-631dd8e86d0d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table streaming_air_quality already exists.\n",
            "Retrieving Project Number...\n",
            "Updated IAM policy for project [mgmt467-fp-11].\n",
            "Creating Subscription openaq-to-bq...\n",
            "\u001b[1;31mERROR:\u001b[0m Failed to create subscription [projects/mgmt467-fp-11/subscriptions/openaq-to-bq]: Resource already exists in the project (resource=openaq-to-bq).\n",
            "\u001b[1;31mERROR:\u001b[0m (gcloud.pubsub.subscriptions.create) Failed to create the following: [openaq-to-bq].\n",
            "Subscription likely already exists.\n",
            "\n",
            "Pipeline Ready: Topic -> Subscription -> Table (streaming_air_quality)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.3: Deploy Open-Meteo Cloud Function (Rome, Italy)\n",
        "import os\n",
        "\n",
        "# 1. Create Directory\n",
        "os.makedirs(\"openaq_function\", exist_ok=True)\n",
        "\n",
        "# 2. Write Function Code (Open-Meteo)\n",
        "main_py_content = \"\"\"import functions_framework\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "import datetime\n",
        "from google.cloud import pubsub_v1\n",
        "\n",
        "publisher = pubsub_v1.PublisherClient()\n",
        "PROJECT_ID = os.environ.get(\"GCP_PROJECT\")\n",
        "TOPIC_ID = \"openaq-topic\"\n",
        "topic_path = publisher.topic_path(PROJECT_ID, TOPIC_ID)\n",
        "\n",
        "@functions_framework.http\n",
        "def fetch_openaq(request):\n",
        "    \\\"\\\"\\\"\n",
        "    Fetches live Air Quality data for Rome, Italy from Open-Meteo.\n",
        "    \\\"\\\"\\\"\n",
        "    # Open-Meteo Air Quality API\n",
        "    url = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
        "\n",
        "    # Coordinates for Rome, Italy\n",
        "    params = {\n",
        "        \"latitude\": 41.9028,\n",
        "        \"longitude\": 12.4964,\n",
        "        \"current\": \"carbon_monoxide,nitrogen_dioxide,ozone\",\n",
        "        \"timezone\": \"Europe/Rome\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code != 200:\n",
        "            return f\"API Error: {response.status_code}\", 500\n",
        "\n",
        "        data = response.json()\n",
        "        current = data.get(\"current\", {})\n",
        "        current_units = data.get(\"current_units\", {})\n",
        "\n",
        "        # Map Open-Meteo names to our standard names\n",
        "        # carbon_monoxide -> co\n",
        "        # nitrogen_dioxide -> no2\n",
        "        # ozone -> o3\n",
        "        mapping = {\n",
        "            \"carbon_monoxide\": \"co\",\n",
        "            \"nitrogen_dioxide\": \"no2\",\n",
        "            \"ozone\": \"o3\"\n",
        "        }\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        # Loop through the 3 gases we requested\n",
        "        for api_name, standard_name in mapping.items():\n",
        "            val = current.get(api_name)\n",
        "            unit = current_units.get(api_name, \"unknown\")\n",
        "\n",
        "            if val is not None:\n",
        "                payload = {\n",
        "                    # Use current UTC time or the time provided by API\n",
        "                    \"timestamp\": current.get(\"time\") + \":00\", # Add seconds for BQ Timestamp format\n",
        "                    \"city\": \"Rome\",\n",
        "                    \"parameter\": standard_name,\n",
        "                    \"value\": float(val),\n",
        "                    \"unit\": unit\n",
        "                }\n",
        "\n",
        "                # Publish\n",
        "                data_str = json.dumps(payload)\n",
        "                publisher.publish(topic_path, data_str.encode(\"utf-8\"))\n",
        "                count += 1\n",
        "\n",
        "        return f\"Success: Published {count} live readings from Rome.\", 200\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", 500\n",
        "\"\"\"\n",
        "\n",
        "with open(\"openaq_function/main.py\", \"w\") as f:\n",
        "    f.write(main_py_content)\n",
        "\n",
        "# 3. Write Requirements\n",
        "with open(\"openaq_function/requirements.txt\", \"w\") as f:\n",
        "    f.write(\"functions-framework==3.*\\ngoogle-cloud-pubsub\\nrequests\\n\")\n",
        "\n",
        "# 4. Deploy\n",
        "print(\"Deploying Open-Meteo Function (Rome)...\")\n",
        "!gcloud functions deploy openaq-ingest \\\n",
        "    --gen2 \\\n",
        "    --runtime=python310 \\\n",
        "    --region=$region \\\n",
        "    --source=./openaq_function \\\n",
        "    --entry-point=fetch_openaq \\\n",
        "    --trigger-http \\\n",
        "    --allow-unauthenticated \\\n",
        "    --set-env-vars=GCP_PROJECT=$project_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdQuOpMGiOaO",
        "outputId": "9d990af8-ca4b-4814-8713-c870c6ac1e03"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deploying Open-Meteo Function (Rome)...\n",
            "You can view your function in the Cloud Console here: https://console.cloud.google.com/functions/details/us-central1/openaq-ingest?project=mgmt467-fp-11\n",
            "\n",
            "buildConfig:\n",
            "  automaticUpdatePolicy: {}\n",
            "  build: projects/906780884112/locations/us-central1/builds/70b47859-e555-48f7-8165-dfedcd0cd4a2\n",
            "  dockerRegistry: ARTIFACT_REGISTRY\n",
            "  dockerRepository: projects/mgmt467-fp-11/locations/us-central1/repositories/gcf-artifacts\n",
            "  entryPoint: fetch_openaq\n",
            "  runtime: python310\n",
            "  serviceAccount: projects/mgmt467-fp-11/serviceAccounts/906780884112-compute@developer.gserviceaccount.com\n",
            "  source:\n",
            "    storageSource:\n",
            "      bucket: gcf-v2-sources-906780884112-us-central1\n",
            "      generation: '1765226112271518'\n",
            "      object: openaq-ingest/function-source.zip\n",
            "  sourceProvenance:\n",
            "    resolvedStorageSource:\n",
            "      bucket: gcf-v2-sources-906780884112-us-central1\n",
            "      generation: '1765226112271518'\n",
            "      object: openaq-ingest/function-source.zip\n",
            "createTime: '2025-12-08T20:35:12.523738923Z'\n",
            "environment: GEN_2\n",
            "labels:\n",
            "  deployment-tool: cli-gcloud\n",
            "name: projects/mgmt467-fp-11/locations/us-central1/functions/openaq-ingest\n",
            "satisfiesPzi: true\n",
            "serviceConfig:\n",
            "  allTrafficOnLatestRevision: true\n",
            "  availableCpu: '0.1666'\n",
            "  availableMemory: 256M\n",
            "  environmentVariables:\n",
            "    GCP_PROJECT: mgmt467-fp-11\n",
            "    LOG_EXECUTION_ID: 'true'\n",
            "  ingressSettings: ALLOW_ALL\n",
            "  maxInstanceCount: 12\n",
            "  maxInstanceRequestConcurrency: 1\n",
            "  revision: openaq-ingest-00001-xan\n",
            "  service: projects/mgmt467-fp-11/locations/us-central1/services/openaq-ingest\n",
            "  serviceAccountEmail: 906780884112-compute@developer.gserviceaccount.com\n",
            "  timeoutSeconds: 60\n",
            "  uri: https://openaq-ingest-jpawmd2vja-uc.a.run.app\n",
            "state: ACTIVE\n",
            "updateTime: '2025-12-08T20:36:30.283589007Z'\n",
            "url: https://us-central1-mgmt467-fp-11.cloudfunctions.net/openaq-ingest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.4: Trigger Ingest & Validate Streaming Data\n",
        "import requests\n",
        "import time\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# 1. Trigger the Cloud Function\n",
        "# We retrieve the URL dynamically from gcloud to make it robust\n",
        "function_url = !gcloud functions describe openaq-ingest --gen2 --region=$region --format=\"value(url)\"\n",
        "function_url = function_url[0]\n",
        "\n",
        "print(f\"Triggering Cloud Function at: {function_url}\")\n",
        "response = requests.get(function_url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(f\"Function Response: {response.text}\")\n",
        "else:\n",
        "    print(f\"Error triggering function: {response.text}\")\n",
        "\n",
        "# 2. Wait for Ingestion\n",
        "print(\"Waiting 15 seconds for data to flow from Pub/Sub to BigQuery...\")\n",
        "time.sleep(15)\n",
        "\n",
        "# 3. Validate Data in BigQuery\n",
        "print(\"Querying BigQuery for latest streaming records...\")\n",
        "\n",
        "query = f\"\"\"\n",
        "    SELECT timestamp, city, parameter, value, unit\n",
        "    FROM `{project_id}.{dataset_name}.{streaming_table_name}`\n",
        "    ORDER BY timestamp DESC\n",
        "    LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "query_job = bq_client.query(query)\n",
        "results = list(query_job.result())\n",
        "\n",
        "if results:\n",
        "    print(f\"\\nSUCCESS: Found {len(results)} records.\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'TIMESTAMP':<25} | {'CITY':<20} | {'PARAM':<10} | {'VALUE'}\")\n",
        "    print(\"-\" * 60)\n",
        "    for row in results:\n",
        "        print(f\"{str(row.timestamp):<25} | {row.city[:20]:<20} | {row.parameter:<10} | {row.value}\")\n",
        "else:\n",
        "    print(\"\\nWARNING: No rows found yet. The pipeline might need a few more seconds.\")\n",
        "    print(\"Try running this cell again in 1 minute.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGE8rAy4lDQa",
        "outputId": "e699721e-d25d-4de4-fa58-0ab26edc1c55"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triggering Cloud Function at: https://us-central1-mgmt467-fp-11.cloudfunctions.net/openaq-ingest\n",
            "Function Response: Success: Published 3 live readings from Rome.\n",
            "Waiting 15 seconds for data to flow from Pub/Sub to BigQuery...\n",
            "Querying BigQuery for latest streaming records...\n",
            "\n",
            "SUCCESS: Found 10 records.\n",
            "------------------------------------------------------------\n",
            "TIMESTAMP                 | CITY                 | PARAM      | VALUE\n",
            "------------------------------------------------------------\n",
            "2025-12-08 22:00:00+00:00 | Rome                 | no2        | 51.4\n",
            "2025-12-08 22:00:00+00:00 | Rome                 | no2        | 51.4\n",
            "2025-12-08 22:00:00+00:00 | Rome                 | co         | 683.0\n",
            "2025-12-08 22:00:00+00:00 | Rome                 | co         | 683.0\n",
            "2025-12-08 22:00:00+00:00 | Rome                 | co         | 683.0\n",
            "2025-12-08 22:00:00+00:00 | Rome                 | no2        | 51.4\n",
            "2025-12-08 22:00:00+00:00 | Rome                 | o3         | 1.0\n",
            "2025-12-08 22:00:00+00:00 | Rome                 | o3         | 1.0\n",
            "2025-12-08 22:00:00+00:00 | Rome                 | co         | 683.0\n",
            "2025-12-08 22:00:00+00:00 | Rome                 | o3         | 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3 - Analytics and Modeling:**"
      ],
      "metadata": {
        "id": "Gogu0-ofhAHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3.0 (Fixed): Feature Engineering (Rush Hour from Time Column)\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# We create a VIEW that calculates new features on the fly.\n",
        "# We fix the error by extracting HOUR from the 'Time' column, not the 'Date' column.\n",
        "\n",
        "fe_view_query = f\"\"\"\n",
        "    CREATE OR REPLACE VIEW `{project_id}.{dataset_name}.engineered_training_data` AS\n",
        "    SELECT\n",
        "        -- TARGET\n",
        "        (CO_GT_ * 1000) as label,\n",
        "\n",
        "        -- ORIGINAL FEATURES\n",
        "        NO2_GT_ as no2,\n",
        "\n",
        "        -- NEW ENGINEERED FEATURES\n",
        "        -- 1. Extract Hour from the 'Time' string (Format is HH.MM.SS)\n",
        "        CASE\n",
        "            WHEN EXTRACT(HOUR FROM PARSE_TIME('%H.%M.%S', Time)) BETWEEN 7 AND 9 THEN 1\n",
        "            WHEN EXTRACT(HOUR FROM PARSE_TIME('%H.%M.%S', Time)) BETWEEN 17 AND 19 THEN 1\n",
        "            ELSE 0\n",
        "        END as is_rush_hour,\n",
        "\n",
        "        -- 2. Is it the Weekend? (Works on Date column: 1=Sun, 7=Sat)\n",
        "        CASE\n",
        "            WHEN EXTRACT(DAYOFWEEK FROM Date) IN (1, 7) THEN 1\n",
        "            ELSE 0\n",
        "        END as is_weekend\n",
        "\n",
        "    FROM\n",
        "        `{project_id}.{dataset_name}.{table_name}`\n",
        "    WHERE\n",
        "        CO_GT_ > -200\n",
        "        AND NO2_GT_ > -200\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Creating Feature Engineering View in {dataset_name}...\")\n",
        "bq_client.query(fe_view_query).result()\n",
        "print(\"✅ Success! View 'engineered_training_data' created with fixed Time parsing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCy4DP0Y6qBk",
        "outputId": "67b69187-d3f5-4c13-bd2c-c5909a2c6289"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Feature Engineering View in air_quality_dataset...\n",
            "✅ Success! View 'engineered_training_data' created with fixed Time parsing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3.1: Train BQML Model (Unit-Corrected)\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Feature Engineering in SQL:\n",
        "# 1. CO_GT_ * 1000 -> Converts mg/m^3 to µg/m^3 (Matching Open-Meteo)\n",
        "# 2. NO2_GT_       -> Already µg/m^3 (Matching Open-Meteo)\n",
        "# 3. Removing O3   -> Removing the incompatible feature\n",
        "\n",
        "train_query = f\"\"\"\n",
        "    CREATE OR REPLACE MODEL `{project_id}.{dataset_name}.air_quality_model_v2`\n",
        "    OPTIONS(model_type='LINEAR_REG') AS\n",
        "    SELECT\n",
        "        (CO_GT_ * 1000) as label,   -- TARGET: Converted to Micrograms\n",
        "        NO2_GT_ as no2              -- FEATURE: Nitrogen Dioxide (True Concentration)\n",
        "    FROM\n",
        "        `{project_id}.{dataset_name}.{table_name}`\n",
        "    WHERE\n",
        "        CO_GT_ > -200\n",
        "        AND NO2_GT_ > -200\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Training Engineered Model (v2) in {project_id}.{dataset_name}...\")\n",
        "job = bq_client.query(train_query)\n",
        "job.result()\n",
        "\n",
        "print(\"SUCCESS: Unit-Corrected Model trained.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EQ82ZGchE8A",
        "outputId": "0600747a-45be-429b-c302-2084f509be0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Engineered Model (v2) in mgmt467-fp-11.air_quality_dataset...\n",
            "SUCCESS: Unit-Corrected Model trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3.12: Train Model with Feature Engineering\n",
        "train_fe_query = f\"\"\"\n",
        "    CREATE OR REPLACE MODEL `{project_id}.{dataset_name}.air_quality_model_fe`\n",
        "    OPTIONS(model_type='LINEAR_REG') AS\n",
        "    SELECT\n",
        "        label,\n",
        "        no2,\n",
        "        is_rush_hour,\n",
        "        is_weekend\n",
        "    FROM\n",
        "        `{project_id}.{dataset_name}.engineered_training_data`\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training Model with Feature Engineering...\")\n",
        "bq_client.query(train_fe_query).result()\n",
        "print(\"✅ Advanced Model Trained!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4HObk777o6D",
        "outputId": "4f746a21-0d2e-4f7c-afaf-d531d001fdab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model with Feature Engineering...\n",
            "✅ Advanced Model Trained!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3.2: Evaluate Model Performance\n",
        "eval_query = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM ML.EVALUATE(MODEL `{project_id}.{dataset_name}.air_quality_model_v2`, (\n",
        "        SELECT\n",
        "            CO_GT_ as label,\n",
        "            NO2_GT_ as no2,\n",
        "            PT08_S5_O3_ as o3\n",
        "        FROM\n",
        "            `{project_id}.{dataset_name}.{table_name}`\n",
        "        WHERE\n",
        "            CO_GT_ > -200 AND NO2_GT_ > -200 AND PT08_S5_O3_ > -200\n",
        "    ))\n",
        "\"\"\"\n",
        "\n",
        "print(\"Running Model Evaluation...\")\n",
        "job = bq_client.query(eval_query)\n",
        "results = list(job.result())\n",
        "\n",
        "print(\"\\n--- Model Metrics ---\")\n",
        "for row in results:\n",
        "    print(f\"R2 Score: {row.r2_score:.4f}\")\n",
        "    print(f\"Mean Absolute Error: {row.mean_absolute_error:.4f} mg/m^3\")\n",
        "    print(f\"Mean Squared Error: {row.mean_squared_error:.4f}\")\n",
        "\n",
        "print(\"\\nInterpretation: An R2 close to 1.0 is perfect. An R2 of 0 means the model is useless.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQKeGZM9hyEr",
        "outputId": "c774574b-bf32-4b70-e4cc-eac861969208"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Model Evaluation...\n",
            "\n",
            "--- Model Metrics ---\n",
            "R2 Score: -2764015.0710\n",
            "Mean Absolute Error: 2187.6278 mg/m^3\n",
            "Mean Squared Error: 5739857.5464\n",
            "\n",
            "Interpretation: An R2 close to 1.0 is perfect. An R2 of 0 means the model is useless.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3.3 (Revised): Predict with Corrected Model\n",
        "predict_query = f\"\"\"\n",
        "    WITH live_features AS (\n",
        "        SELECT\n",
        "            timestamp,\n",
        "            -- We only need NO2 now\n",
        "            MAX(CASE WHEN parameter = 'no2' THEN value END) as no2,\n",
        "            MAX(CASE WHEN parameter = 'co' THEN value END) as actual_co_live\n",
        "        FROM `{project_id}.{dataset_name}.{streaming_table_name}`\n",
        "        GROUP BY timestamp\n",
        "        HAVING no2 IS NOT NULL\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "        timestamp,\n",
        "        ROUND(predicted_label, 2) as predicted_co_ug, -- Prediction in Micrograms\n",
        "        actual_co_live,\n",
        "        no2 as input_no2\n",
        "    FROM\n",
        "        ML.PREDICT(MODEL `{project_id}.{dataset_name}.air_quality_model_v2`,\n",
        "        (SELECT * FROM live_features))\n",
        "    ORDER BY timestamp DESC\n",
        "    LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "print(\"Running Predictions using Engineered Model...\")\n",
        "job = bq_client.query(predict_query)\n",
        "results = list(job.result())\n",
        "\n",
        "print(\"\\n--- Live Predictions (Corrected Scale) ---\")\n",
        "print(f\"{'TIMESTAMP':<25} | {'PREDICTED CO (µg)':<18} | {'ACTUAL CO':<10} | {'INPUT NO2'}\")\n",
        "print(\"-\" * 75)\n",
        "for row in results:\n",
        "    print(f\"{str(row.timestamp):<25} | {row.predicted_co_ug:<18} | {row.actual_co_live:<10} | {row.input_no2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlhN2UddiGIC",
        "outputId": "fe09fbbe-c571-4ba7-f271-308c7cf63005"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Predictions using Engineered Model...\n",
            "\n",
            "--- Live Predictions (Corrected Scale) ---\n",
            "TIMESTAMP                 | PREDICTED CO (µg)  | ACTUAL CO  | INPUT NO2\n",
            "---------------------------------------------------------------------------\n",
            "2025-12-09 16:00:00+00:00 | 814.07             | 300.0      | 47.1\n",
            "2025-12-09 15:00:00+00:00 | 439.12             | 268.0      | 28.9\n",
            "2025-12-09 14:00:00+00:00 | 354.65             | 252.0      | 24.8\n",
            "2025-12-09 13:00:00+00:00 | 239.28             | 250.0      | 19.2\n",
            "2025-12-09 12:00:00+00:00 | 299.03             | 277.0      | 22.1\n",
            "2025-12-09 11:00:00+00:00 | 329.93             | 318.0      | 23.6\n",
            "2025-12-09 10:00:00+00:00 | 406.16             | 363.0      | 27.3\n",
            "2025-12-09 09:00:00+00:00 | 505.04             | 371.0      | 32.1\n",
            "2025-12-09 08:00:00+00:00 | 550.37             | 396.0      | 34.3\n",
            "2025-12-09 07:00:00+00:00 | 517.4              | 390.0      | 32.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3.32: Predict using Feature Engineered Model\n",
        "# We now calculate 'is_rush_hour' and 'is_weekend' dynamically from the streaming timestamps.\n",
        "\n",
        "predict_fe_query = f\"\"\"\n",
        "    WITH live_features AS (\n",
        "        SELECT\n",
        "            timestamp,\n",
        "            -- 1. Extract the raw NO2 reading (Feature 1)\n",
        "            MAX(CASE WHEN parameter = 'no2' THEN value END) as no2,\n",
        "\n",
        "            -- Optional: Get actual CO if available for comparison\n",
        "            MAX(CASE WHEN parameter = 'co' THEN value END) as actual_co_live,\n",
        "\n",
        "            -- 2. Engineer \"Rush Hour\" dynamically (Feature 2)\n",
        "            -- We assume the timestamp is in the correct timezone (or close enough for this demo)\n",
        "            CASE\n",
        "                WHEN EXTRACT(HOUR FROM timestamp) BETWEEN 7 AND 9 THEN 1\n",
        "                WHEN EXTRACT(HOUR FROM timestamp) BETWEEN 17 AND 19 THEN 1\n",
        "                ELSE 0\n",
        "            END as is_rush_hour,\n",
        "\n",
        "            -- 3. Engineer \"Weekend\" dynamically (Feature 3)\n",
        "            -- BigQuery: Sunday=1, Saturday=7\n",
        "            CASE\n",
        "                WHEN EXTRACT(DAYOFWEEK FROM timestamp) IN (1, 7) THEN 1\n",
        "                ELSE 0\n",
        "            END as is_weekend\n",
        "\n",
        "        FROM `{project_id}.{dataset_name}.{streaming_table_name}`\n",
        "        GROUP BY timestamp\n",
        "        HAVING no2 IS NOT NULL\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "        timestamp,\n",
        "        ROUND(predicted_label, 2) as predicted_co_ug, -- The Prediction\n",
        "        actual_co_live as actual_co,\n",
        "        no2 as input_no2,\n",
        "        is_rush_hour,\n",
        "        is_weekend\n",
        "    FROM\n",
        "        ML.PREDICT(MODEL `{project_id}.{dataset_name}.air_quality_model_fe`,\n",
        "        (SELECT * FROM live_features))\n",
        "    ORDER BY timestamp DESC\n",
        "    LIMIT 15\n",
        "\"\"\"\n",
        "\n",
        "print(\"Running Predictions using Feature Engineered Model...\")\n",
        "job = bq_client.query(predict_fe_query)\n",
        "results = list(job.result())\n",
        "\n",
        "print(\"\\n--- Live Feature Engineering Predictions ---\")\n",
        "# Header\n",
        "print(f\"{'TIMESTAMP':<22} | {'PRED CO(µg)':<12} | {'ACTUAL':<8} | {'NO2':<6} | {'RUSH?':<5} | {'WKND?'}\")\n",
        "print(\"-\" * 85)\n",
        "\n",
        "for row in results:\n",
        "    # Handle None for actuals nicely\n",
        "    actual = row.actual_co if row.actual_co is not None else \"N/A\"\n",
        "\n",
        "    print(f\"{str(row.timestamp)[:19]:<22} | {row.predicted_co_ug:<12} | {str(actual):<8} | {row.input_no2:<6} | {row.is_rush_hour:<5} | {row.is_weekend}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1r0rhbW8KQD",
        "outputId": "204db9a3-a184-4177-b2ed-2f76524b415f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Predictions using Feature Engineered Model...\n",
            "\n",
            "--- Live Feature Engineering Predictions ---\n",
            "TIMESTAMP              | PRED CO(µg)  | ACTUAL   | NO2    | RUSH? | WKND?\n",
            "-------------------------------------------------------------------------------------\n",
            "2025-12-09 22:00:00    | 991.12       | 642.0    | 54.0   | 0     | 0\n",
            "2025-12-09 21:00:00    | 1139.46      | 657.0    | 61.8   | 0     | 0\n",
            "2025-12-09 20:00:00    | 1242.15      | 615.0    | 67.2   | 0     | 0\n",
            "2025-12-09 19:00:00    | 1903.43      | 536.0    | 69.9   | 1     | 0\n",
            "2025-12-09 18:00:00    | 1966.19      | 416.0    | 73.2   | 1     | 0\n",
            "2025-12-09 17:00:00    | 1933.86      | 383.0    | 71.5   | 1     | 0\n",
            "2025-12-09 16:00:00    | 859.9        | 300.0    | 47.1   | 0     | 0\n",
            "2025-12-09 15:00:00    | 513.78       | 268.0    | 28.9   | 0     | 0\n",
            "2025-12-09 14:00:00    | 435.8        | 252.0    | 24.8   | 0     | 0\n",
            "2025-12-09 13:00:00    | 329.3        | 250.0    | 19.2   | 0     | 0\n",
            "2025-12-09 12:00:00    | 384.46       | 277.0    | 22.1   | 0     | 0\n",
            "2025-12-09 11:00:00    | 412.98       | 318.0    | 23.6   | 0     | 0\n",
            "2025-12-09 10:00:00    | 483.35       | 363.0    | 27.3   | 0     | 0\n",
            "2025-12-09 09:00:00    | 1184.57      | 371.0    | 32.1   | 1     | 0\n",
            "2025-12-09 08:00:00    | 1226.4       | 396.0    | 34.3   | 1     | 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3.4: The Showdown - Base Model vs. Feature Engineered Model\n",
        "from google.cloud import bigquery\n",
        "\n",
        "print(\"=Running Model Comparison\\n\")\n",
        "\n",
        "# --- 1. Evaluate Base Model (Only NO2) ---\n",
        "# Note: BQML is smart enough to ignore the extra columns in the view that the base model doesn't need.\n",
        "base_eval_query = f\"\"\"\n",
        "    SELECT\n",
        "        r2_score,\n",
        "        mean_absolute_error,\n",
        "        mean_squared_error\n",
        "    FROM\n",
        "        ML.EVALUATE(MODEL `{project_id}.{dataset_name}.air_quality_model_v2`,\n",
        "        (SELECT label, no2 FROM `{project_id}.{dataset_name}.engineered_training_data`))\n",
        "\"\"\"\n",
        "\n",
        "base_job = bq_client.query(base_eval_query)\n",
        "base_results = list(base_job.result())[0]\n",
        "\n",
        "# --- 2. Evaluate Feature Engineered Model (NO2 + Rush Hour + Weekend) ---\n",
        "fe_eval_query = f\"\"\"\n",
        "    SELECT\n",
        "        r2_score,\n",
        "        mean_absolute_error,\n",
        "        mean_squared_error\n",
        "    FROM\n",
        "        ML.EVALUATE(MODEL `{project_id}.{dataset_name}.air_quality_model_fe`,\n",
        "        (SELECT label, no2, is_rush_hour, is_weekend FROM `{project_id}.{dataset_name}.engineered_training_data`))\n",
        "\"\"\"\n",
        "\n",
        "fe_job = bq_client.query(fe_eval_query)\n",
        "fe_results = list(fe_job.result())[0]\n",
        "\n",
        "# --- 3. Print the Scorecard ---\n",
        "print(f\"{'METRIC':<25} | {'BASE MODEL':<15} | {'FE MODEL (NEW)':<15} | {'IMPROVEMENT'}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "metrics = [\n",
        "    (\"R2 Score (Higher is better)\", base_results.r2_score, fe_results.r2_score),\n",
        "    (\"MAE (Lower is better)\", base_results.mean_absolute_error, fe_results.mean_absolute_error),\n",
        "    (\"MSE (Lower is better)\", base_results.mean_squared_error, fe_results.mean_squared_error)\n",
        "]\n",
        "\n",
        "for name, base, fe in metrics:\n",
        "    # Calculate simple percent difference\n",
        "    diff = fe - base\n",
        "    if \"R2\" in name:\n",
        "        color = \"✅\" if diff > 0 else \"❌\"\n",
        "    else:\n",
        "        color = \"✅\" if diff < 0 else \"❌\" # For Error, negative diff is good\n",
        "\n",
        "    print(f\"{name:<25} | {base:<15.4f} | {fe:<15.4f} | {color}\")\n",
        "\n",
        "print(\"\\n---------------------------------------------------------------------------\")\n",
        "if fe_results.r2_score > base_results.r2_score:\n",
        "    print(\"The Feature Engineered Model is superior.\")\n",
        "    print(\"Analysis: Adding 'Rush Hour' and 'Weekend' flags helped explain the variance.\")\n",
        "else:\n",
        "    print(\"The Feature Engineering didn't add much predictive power.\")\n",
        "    print(\"Analysis: NO2 might be such a strong predictor that time of day doesn't matter much more.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-teq_7aU8bJo",
        "outputId": "0f29f176-ef02-4d64-d468-95be65099f87"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=Running Model Comparison\n",
            "\n",
            "METRIC                    | BASE MODEL      | FE MODEL (NEW)  | IMPROVEMENT\n",
            "---------------------------------------------------------------------------\n",
            "R2 Score (Higher is better) | 0.4669          | 0.5096          | ✅\n",
            "MAE (Lower is better)     | 741.9736        | 722.8243        | ✅\n",
            "MSE (Lower is better)     | 1133450.4257    | 1042762.7861    | ✅\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "The Feature Engineered Model is superior.\n",
            "Analysis: Adding 'Rush Hour' and 'Weekend' flags helped explain the variance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4.1: Create 'Current_Air_Quality' View\n",
        "# This view pivots the latest data so Looker Studio sees simple columns:\n",
        "# timestamp | co | no2 | o3\n",
        "\n",
        "view_query = f\"\"\"\n",
        "    CREATE OR REPLACE VIEW `{project_id}.{dataset_name}.current_air_quality_view` AS\n",
        "    SELECT\n",
        "        timestamp,\n",
        "        -- Pivot rows into columns for easier Dashboarding\n",
        "        MAX(CASE WHEN parameter = 'co' THEN value END) as co_level,\n",
        "        MAX(CASE WHEN parameter = 'no2' THEN value END) as no2_level,\n",
        "        MAX(CASE WHEN parameter = 'o3' THEN value END) as o3_level,\n",
        "        MAX(CASE WHEN parameter = 'co' THEN unit END) as unit\n",
        "    FROM `{project_id}.{dataset_name}.{streaming_table_name}`\n",
        "    GROUP BY timestamp\n",
        "    -- Only get the most recent timestamp\n",
        "    ORDER BY timestamp DESC\n",
        "    LIMIT 1\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Creating View: {project_id}.{dataset_name}.current_air_quality_view...\")\n",
        "job = bq_client.query(view_query)\n",
        "job.result()\n",
        "\n",
        "print(\"Success! View created. This will always contain only the single latest record.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k-Rh1j6tSwt",
        "outputId": "506fed16-7a3c-4f72-ced7-f4225f3cf72d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating View: mgmt467-fp-11.air_quality_dataset.current_air_quality_view...\n",
            "Success! View created. This will always contain only the single latest record.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4.2: Create 'Recent_24h_Correlation' View\n",
        "correlation_view_query = f\"\"\"\n",
        "    CREATE OR REPLACE VIEW `{project_id}.{dataset_name}.recent_24h_correlation_view` AS\n",
        "    SELECT\n",
        "        timestamp,\n",
        "        MAX(CASE WHEN parameter = 'no2' THEN value END) as no2_level,\n",
        "        MAX(CASE WHEN parameter = 'o3' THEN value END) as o3_level\n",
        "    FROM `{project_id}.{dataset_name}.{streaming_table_name}`\n",
        "    WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)\n",
        "    GROUP BY timestamp\n",
        "    -- Ensure we only keep rows where BOTH sensors reported data\n",
        "    HAVING no2_level IS NOT NULL AND o3_level IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Creating View: {project_id}.{dataset_name}.recent_24h_correlation_view...\")\n",
        "job = bq_client.query(correlation_view_query)\n",
        "job.result()\n",
        "\n",
        "print(\"Success! View created. Ready for Scatter Plot visualization.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9vevB30viST",
        "outputId": "30da359e-3a38-4c9a-e0b9-985241ca4615"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating View: mgmt467-fp-11.air_quality_dataset.recent_24h_correlation_view...\n",
            "Success! View created. Ready for Scatter Plot visualization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4.3: Create 'Historical_Reliability' View\n",
        "reliability_view_query = f\"\"\"\n",
        "    CREATE OR REPLACE VIEW `{project_id}.{dataset_name}.historical_reliability_view` AS\n",
        "    SELECT\n",
        "        -- Calculate Total Rows\n",
        "        COUNT(*) as total_records,\n",
        "\n",
        "        -- Calculate Reliability % for Carbon Monoxide\n",
        "        ROUND((COUNTIF(CO_GT_ > -200) / COUNT(*)) * 100, 2) as co_reliability_pct,\n",
        "\n",
        "        -- Calculate Reliability % for Nitrogen Dioxide\n",
        "        ROUND((COUNTIF(NO2_GT_ > -200) / COUNT(*)) * 100, 2) as no2_reliability_pct,\n",
        "\n",
        "        -- Calculate Overall System Reliability (Average of the two)\n",
        "        ROUND(\n",
        "            (\n",
        "                (COUNTIF(CO_GT_ > -200) + COUNTIF(NO2_GT_ > -200))\n",
        "                / (COUNT(*) * 2)\n",
        "            ) * 100,\n",
        "        2) as overall_reliability_pct\n",
        "\n",
        "    FROM `{project_id}.{dataset_name}.{table_name}`\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Creating View: {project_id}.{dataset_name}.historical_reliability_view...\")\n",
        "job = bq_client.query(reliability_view_query)\n",
        "job.result()\n",
        "\n",
        "print(\"Success! View created correctly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gOO1p_awS_s",
        "outputId": "6d2f44a6-61df-4a4b-988f-818fcb11cba9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating View: mgmt467-fp-11.air_quality_dataset.historical_reliability_view...\n",
            "Success! View created correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4.4: Create 'Pollution_Trend_Comparison' View\n",
        "# This complex query does two things:\n",
        "# 1. Grabs the last 6 hours of Live Data (Rome 2025)\n",
        "# 2. Joins it with the Average Baseline from 2004 (Historical)\n",
        "\n",
        "time_series_query = f\"\"\"\n",
        "    CREATE OR REPLACE VIEW `{project_id}.{dataset_name}.pollution_trend_view` AS\n",
        "\n",
        "    WITH live_data AS (\n",
        "        SELECT\n",
        "            timestamp,\n",
        "            MAX(CASE WHEN parameter = 'co' THEN value END) as live_co_ug,\n",
        "            MAX(CASE WHEN parameter = 'no2' THEN value END) as live_no2_ug\n",
        "        FROM `{project_id}.{dataset_name}.{streaming_table_name}`\n",
        "        WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 6 HOUR)\n",
        "        GROUP BY timestamp\n",
        "    ),\n",
        "\n",
        "    historical_baseline AS (\n",
        "        -- Calculate the average CO/NO2 from 2004 to draw a \"baseline\" line\n",
        "        -- Remember: Historical CO was mg/m3, so we * 1000 to match live units\n",
        "        SELECT\n",
        "            AVG(CO_GT_) * 1000 as avg_historical_co,\n",
        "            AVG(NO2_GT_) as avg_historical_no2\n",
        "        FROM `{project_id}.{dataset_name}.{table_name}`\n",
        "        WHERE CO_GT_ > -200 AND NO2_GT_ > -200\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "        l.timestamp,\n",
        "        l.live_co_ug,\n",
        "        l.live_no2_ug,\n",
        "        h.avg_historical_co as baseline_co_2004,\n",
        "        h.avg_historical_no2 as baseline_no2_2004\n",
        "    FROM live_data l\n",
        "    CROSS JOIN historical_baseline h\n",
        "    ORDER BY l.timestamp ASC\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Creating View: {project_id}.{dataset_name}.pollution_trend_view...\")\n",
        "job = bq_client.query(time_series_query)\n",
        "job.result()\n",
        "\n",
        "print(\"Success! Time-Series View created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX9aGxAHyT78",
        "outputId": "82bb7e13-edcd-42e5-d10c-3fd9cf869108"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating View: mgmt467-fp-11.air_quality_dataset.pollution_trend_view...\n",
            "Success! Time-Series View created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4.4: Create Smoothed Pollution Trend View\n",
        "time_series_query = f\"\"\"\n",
        "    CREATE OR REPLACE VIEW `{project_id}.{dataset_name}.pollution_trend_view` AS\n",
        "\n",
        "    WITH live_data_smoothed AS (\n",
        "        SELECT\n",
        "            -- Truncate timestamp to the nearest Minute to prevent \"Too Many Rows\" error\n",
        "            TIMESTAMP_TRUNC(timestamp, MINUTE) as timestamp_minute,\n",
        "            AVG(CASE WHEN parameter = 'co' THEN value END) as live_co_ug,\n",
        "            AVG(CASE WHEN parameter = 'no2' THEN value END) as live_no2_ug\n",
        "        FROM `{project_id}.{dataset_name}.{streaming_table_name}`\n",
        "        -- Focus on last 24 hours to keep chart clean\n",
        "        WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)\n",
        "        GROUP BY 1\n",
        "    ),\n",
        "\n",
        "    historical_baseline AS (\n",
        "        SELECT\n",
        "            AVG(CO_GT_) * 1000 as avg_historical_co,\n",
        "            AVG(NO2_GT_) as avg_historical_no2\n",
        "        FROM `{project_id}.{dataset_name}.{table_name}`\n",
        "        WHERE CO_GT_ > -200 AND NO2_GT_ > -200\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "        l.timestamp_minute as timestamp,\n",
        "        l.live_co_ug,\n",
        "        l.live_no2_ug,\n",
        "        h.avg_historical_co as baseline_co_2004,\n",
        "        h.avg_historical_no2 as baseline_no2_2004\n",
        "    FROM live_data_smoothed l\n",
        "    CROSS JOIN historical_baseline h\n",
        "    ORDER BY l.timestamp_minute ASC\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Updating View: {project_id}.{dataset_name}.pollution_trend_view...\")\n",
        "job = bq_client.query(time_series_query)\n",
        "job.result()\n",
        "\n",
        "print(\"Success! View updated with 1-minute smoothing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSimujbk2kbD",
        "outputId": "dd368efd-99d7-43a0-a558-af09f2335bb8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating View: mgmt467-fp-11.air_quality_dataset.pollution_trend_view...\n",
            "Success! View updated with 1-minute smoothing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4.5: Create 'Forecast_Accuracy_View'\n",
        "accuracy_view_query = f\"\"\"\n",
        "    CREATE OR REPLACE VIEW `{project_id}.{dataset_name}.forecast_accuracy_view` AS\n",
        "\n",
        "    WITH live_features AS (\n",
        "        SELECT\n",
        "            -- Truncate to minute to align with our smoothed data\n",
        "            TIMESTAMP_TRUNC(timestamp, MINUTE) as timestamp,\n",
        "            AVG(CASE WHEN parameter = 'no2' THEN value END) as no2,\n",
        "            AVG(CASE WHEN parameter = 'co' THEN value END) as actual_co\n",
        "        FROM `{project_id}.{dataset_name}.{streaming_table_name}`\n",
        "        GROUP BY 1\n",
        "        HAVING no2 IS NOT NULL AND actual_co IS NOT NULL\n",
        "    ),\n",
        "\n",
        "    predictions AS (\n",
        "        SELECT\n",
        "            timestamp,\n",
        "            predicted_label as predicted_co,\n",
        "            actual_co,\n",
        "            no2 as input_no2\n",
        "        FROM\n",
        "            ML.PREDICT(MODEL `{project_id}.{dataset_name}.air_quality_model_v2`,\n",
        "            (SELECT * FROM live_features))\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "        timestamp,\n",
        "        ROUND(predicted_co, 2) as predicted_co,\n",
        "        ROUND(actual_co, 2) as actual_co,\n",
        "        -- Calculate Error Percentage: ABS(Pred - Actual) / Actual\n",
        "        ROUND(\n",
        "            ABS(predicted_co - actual_co) / NULLIF(actual_co, 0) * 100\n",
        "        , 2) as error_pct\n",
        "    FROM predictions\n",
        "    ORDER BY timestamp DESC\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Creating View: {project_id}.{dataset_name}.forecast_accuracy_view...\")\n",
        "job = bq_client.query(accuracy_view_query)\n",
        "job.result()\n",
        "\n",
        "print(\"Success! View created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s50y94CxqPY",
        "outputId": "6fc76f0c-ec9e-44e6-cbb7-e24211460bea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating View: mgmt467-fp-11.air_quality_dataset.forecast_accuracy_view...\n",
            "Success! View created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 5: Enable API & Create Scheduler\n",
        "import time\n",
        "\n",
        "# 1. Enable Cloud Scheduler API\n",
        "print(\"Enabling Cloud Scheduler API...\")\n",
        "!gcloud services enable cloudscheduler.googleapis.com\n",
        "print(\"API Enabled. Waiting 30 seconds for propagation...\")\n",
        "time.sleep(30) # Important: API enablement takes a moment to sync\n",
        "\n",
        "# 2. Get Function URL\n",
        "function_url_list = !gcloud functions describe openaq-ingest --gen2 --region=$region --format=\"value(url)\"\n",
        "function_url = function_url_list[0]\n",
        "print(f\"Target URL: {function_url}\")\n",
        "\n",
        "# 3. Create Scheduler Job\n",
        "job_name = \"openaq-ticker\"\n",
        "schedule = \"*/15 * * * *\"\n",
        "\n",
        "print(f\"Creating Scheduler Job '{job_name}'...\")\n",
        "\n",
        "# Try creating, if it exists, update it\n",
        "!gcloud scheduler jobs create http $job_name \\\n",
        "    --schedule=\"$schedule\" \\\n",
        "    --uri=\"$function_url\" \\\n",
        "    --http-method=GET \\\n",
        "    --location=$region \\\n",
        "    --quiet \\\n",
        "    || gcloud scheduler jobs update http $job_name \\\n",
        "    --schedule=\"$schedule\" \\\n",
        "    --uri=\"$function_url\" \\\n",
        "    --http-method=GET \\\n",
        "    --location=$region \\\n",
        "    --quiet\n",
        "\n",
        "print(\"\\nSUCCESS: Scheduler created. Automation is live.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHUJoCwQ-H9c",
        "outputId": "72d79574-deff-4275-a87e-c12147086be5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enabling Cloud Scheduler API...\n",
            "Operation \"operations/acf.p2-906780884112-fc4e671c-2fde-4528-8aed-f56b03211d52\" finished successfully.\n",
            "API Enabled. Waiting 30 seconds for propagation...\n",
            "Target URL: https://us-central1-mgmt467-fp-11.cloudfunctions.net/openaq-ingest\n",
            "Creating Scheduler Job 'openaq-ticker'...\n",
            "attemptDeadline: 180s\n",
            "httpTarget:\n",
            "  headers:\n",
            "    User-Agent: Google-Cloud-Scheduler\n",
            "  httpMethod: GET\n",
            "  uri: https://us-central1-mgmt467-fp-11.cloudfunctions.net/openaq-ingest\n",
            "name: projects/mgmt467-fp-11/locations/us-central1/jobs/openaq-ticker\n",
            "retryConfig:\n",
            "  maxBackoffDuration: 3600s\n",
            "  maxDoublings: 5\n",
            "  maxRetryDuration: 0s\n",
            "  minBackoffDuration: 5s\n",
            "schedule: '*/15 * * * *'\n",
            "scheduleTime: '2025-12-08T22:15:00Z'\n",
            "state: ENABLED\n",
            "status:\n",
            "  code: -1\n",
            "timeZone: Etc/UTC\n",
            "userUpdateTime: '2025-12-08T22:02:24.435844Z'\n",
            "\n",
            "SUCCESS: Scheduler created. Automation is live.\n"
          ]
        }
      ]
    }
  ]
}